{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Group HOMEWORK**. This final project can be collaborative. The maximum members of a group is 2. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints), you can determine: \n",
    "-  how to preprocess the input text (e.g., remove emoji, remove stopwords, text lemmatization and stemming, etc.);\n",
    "-  which method to use to encode text features (e.g., TF-IDF, N-grams, Word2vec, GloVe, Part-of-Speech (POS), etc.);\n",
    "-  which model to use.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision, Accuracy and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. \n",
    "- **Format**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary and answer the following questions: \n",
    "    - What preprocessing steps do you follow?\n",
    "    - How do you select the features from the inputs? \n",
    "    - Which model you use and what is the structure of your model?\n",
    "    - How do you train your model?\n",
    "    - What is the performance of your best model?\n",
    "    - What other models or feature engineering methods would you like to implement in the future?\n",
    "- **Two Rules**, violations will result in 0 points in the grade: \n",
    "    - Not allowed to use test set in the training: You CANNOT use any of the instances from test set in the training process. \n",
    "    - Not allowed to use code from generative AI (e.g., ChatGPT). \n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above.\n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report. \n",
    "\n",
    "The report should include: (a)code, (b)outputs, (c)explainations for each step, and (d)summary (you can add markdown cells). \n",
    "\n",
    "The due date is **December 8, Friday by 11:59pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7caf28-1c77-4f87-8a62-7f3a7f79e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/miltonrue/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/miltonrue/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import everything we need\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f516ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the test and train dataframes, and spilling into X(text) and Y(label)\n",
    "comments_df = pd.read_csv('edos_labelled_data.csv') \n",
    "le = preprocessing.LabelEncoder()\n",
    "comments_df[\"label\"] = le.fit_transform(comments_df[\"label\"])\n",
    "comments_train_df = comments_df[comments_df[\"split\"] == \"train\"]\n",
    "comments_test_df = comments_df[comments_df[\"split\"] == \"test\"]\n",
    "X_train, X_test = train_test_split(comments_train_df[\"text\"], shuffle=False)\n",
    "Y_train, Y_test = train_test_split(comments_train_df[\"label\"], shuffle=False)\n",
    "X_test_final = comments_test_df[\"text\"]\n",
    "Y_test_final = comments_test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fde145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comments: list[str]) -> list[str]:\n",
    "    # Removes unicode characters and punctuation and makes everything lowercase\n",
    "    # Returns a list of \"cleaned\" strings\n",
    "    wln = WordNetLemmatizer()\n",
    "    comments_clean = [comment.encode(\"ascii\", \"ignore\").decode() for comment in comments]\n",
    "    comments_clean = list(map(lambda x : x.lower(), comments_clean))\n",
    "    comments_clean = [re.sub(r'(#\\w+|\\[user\\]|\\[url\\])', '', comment) for comment in comments_clean]\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    comments_clean = [comment.translate(translator) for comment in comments_clean]\n",
    "\n",
    "    return comments_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc398840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    # Convert pos_tag output to a part of speech recognized by the lemmatizer\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_and_lemmatize(x):\n",
    "    clean_x = clean(x)\n",
    "    # In clean2 we want to use the lemmatizer to simplify the data.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    out = []\n",
    "    for sentence in clean_x:\n",
    "        tokens = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        tagged = list(map(\n",
    "                lambda x : (x[0], get_wordnet_pos(x[1])),\n",
    "                tokens\n",
    "        ))\n",
    "        # Remove all words that are not recognized parts of speech\n",
    "        word_and_pos = list(filter(\n",
    "            lambda x : x[1] != '', \n",
    "            tagged\n",
    "        ))\n",
    "        # Map the lemmatizer over each word in the sentence\n",
    "        lemmatized_words = list(\n",
    "            map(\n",
    "                lambda x : lemmatizer.lemmatize(x[0], x[1]),\n",
    "                word_and_pos\n",
    "            )\n",
    "        )\n",
    "        # Join lemmatized words back into a single string\n",
    "        lemmatized_sentence = \" \".join(lemmatized_words)\n",
    "        out.append(lemmatized_sentence)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b163c0",
   "metadata": {},
   "source": [
    "\n",
    "### Encoding 1\n",
    "The first encoding method was basically just cleaning the data and converting sentences into the number of times each word ocurred in that sentence using the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a23c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the s on the X_train data\n",
    "vectorizer = CountVectorizer()\n",
    "out = clean_and_lemmatize(X_train)\n",
    "vectorizer.fit(out)\n",
    "\n",
    "def word_frequency(x : pd.DataFrame) -> pd.DataFrame:\n",
    "    out = vectorizer.transform(clean_and_lemmatize(x))\n",
    "    df = pd.DataFrame(out.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return df\n",
    "\n",
    "\n",
    "train_freq = word_frequency(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2349a",
   "metadata": {},
   "source": [
    "### Encoding 2\n",
    "The second encoding method treated the training comments labeled 'sexist' as a body of text on which we fit the TF_IDF vectorizer.\n",
    "\n",
    "Conceptually, the idea was that the TF_IDF would identify words that were semantically important in sexist comments and would encode this in the features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2442d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_VEC = TfidfVectorizer()\n",
    "# Fit the TF_IDF vectorizer on the data labeled sexist.\n",
    "TF_IDF_VEC.fit(clean_and_lemmatize(X_train[Y_train==1]))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDF(x):\n",
    "    cleaned = clean_and_lemmatize(x)\n",
    "    out = TF_IDF_VEC.transform(cleaned)\n",
    "    df = pd.DataFrame(out.toarray(), columns=TF_IDF_VEC.get_feature_names_out())\n",
    "    return df\n",
    "train_tf_idf = TFIDF(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ab0b4",
   "metadata": {},
   "source": [
    "## Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe14d1a",
   "metadata": {},
   "source": [
    "### Model 1: Word Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1820237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first model checks if a comment contains a word in the bad_words list\n",
    "# If it does, the comments is decided to be sexist, if not the comment is decided to be non-sexist\n",
    "def model_1(sentence : str, bad_words):\n",
    "    for word in sentence.split():\n",
    "        if word in bad_words:\n",
    "            return 1  \n",
    "    return 0\n",
    "\n",
    "\n",
    "# Find the best cutoff for bad_words list\n",
    "def optimize_model_1(word_frequency : pd.DataFrame):\n",
    "    freq_with_labels = word_frequency.copy()\n",
    "    freq_with_labels['_label'] = Y_train.tolist()\n",
    "    # sexist_comment_percent = the percentage of comments that are sexist (as a decimal)\n",
    "    sexist_comment_percent = len(Y_train.loc[Y_train== 1]) / len(Y_train)\n",
    "    # common_freq = all words that appear at least 5 times\n",
    "    common_freq = freq_with_labels.sum().loc[freq_with_labels.sum() >= 5]\n",
    "    # sexist = the number of times a word appears in a sexist comment given for all words in common_freq\n",
    "    sexist = word_frequency[freq_with_labels[\"_label\"] == 1].sum().loc[freq_with_labels.sum() >= 5]\n",
    "    # ratio = # of sexist comments / # of comments for all words in common_freq\n",
    "    ratio  = (sexist/common_freq).sort_values()\n",
    "    \n",
    "    f1_scores = {}\n",
    "    clean_X_test = clean_and_lemmatize(X_test)\n",
    "    \n",
    "    for i in range(-50, 50):\n",
    "        # All words with a sexism ratio that exceeds the non-sexist comment percentage by at least i / 100 \n",
    "        bad_words = ratio.loc[ratio > 1 - sexist_comment_percent + i / 100].index\n",
    "\n",
    "        predict = []\n",
    "        for comment in clean_X_test:\n",
    "            predict.append(model_1(comment, bad_words))\n",
    "        f1 = f1_score(Y_test, predict, average = \"weighted\") \n",
    "        f1_scores[i] = f1\n",
    "        \n",
    "    print(max(f1_scores.items(), key=lambda x:x[1]))\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aca5527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-6, 0.7352199920258286)\n",
      "{-50: 0.17804317499534933, -49: 0.17804317499534933, -48: 0.17968117307206796, -47: 0.18567282530301088, -46: 0.18567282530301088, -45: 0.18567282530301088, -44: 0.18964148583270038, -43: 0.19161815310885685, -42: 0.19358974222703249, -41: 0.19947426718753286, -40: 0.21261201012925177, -39: 0.22547670441252593, -38: 0.28757016222522214, -37: 0.3288639391834622, -36: 0.3590275269555744, -35: 0.39923649497536207, -34: 0.43314129069834084, -33: 0.4468503152425395, -32: 0.4628584342190909, -31: 0.5020683316800623, -30: 0.5326052874667365, -29: 0.5622335608140632, -28: 0.6127656857163015, -27: 0.6249041647838623, -26: 0.6348777446123554, -25: 0.6594079839679615, -24: 0.6649314016948813, -23: 0.6780798588160201, -22: 0.6803215442389835, -21: 0.6948059503918657, -20: 0.6948059503918657, -19: 0.6955590039846019, -18: 0.7038616043803794, -17: 0.70505810779471, -16: 0.7143255709302139, -15: 0.7240687721925245, -14: 0.7298515027027545, -13: 0.7298515027027545, -12: 0.7317460476835401, -11: 0.7324096394373467, -10: 0.7324096394373467, -9: 0.7324096394373467, -8: 0.7323514360947434, -7: 0.7319113162380683, -6: 0.7352199920258286, -5: 0.7352199920258286, -4: 0.7246190993991397, -3: 0.7246190993991397, -2: 0.7246190993991397, -1: 0.7262549416273042, 0: 0.7262549416273042, 1: 0.7266908525507811, 2: 0.7266908525507811, 3: 0.7239628963339005, 4: 0.7214653421284221, 5: 0.7214653421284221, 6: 0.7214653421284221, 7: 0.7222773103840954, 8: 0.7222773103840954, 9: 0.723189332755918, 10: 0.7045013433204822, 11: 0.7038233275994041, 12: 0.7038233275994041, 13: 0.7061958025248807, 14: 0.7061958025248807, 15: 0.7047168384495072, 16: 0.6873755336528473, 17: 0.6896813291570203, 18: 0.6896813291570203, 19: 0.6896813291570203, 20: 0.6600864751257977, 21: 0.6534372454922821, 22: 0.6534372454922821, 23: 0.6534372454922821, 24: 0.6534372454922821, 25: 0.6534372454922821, 26: 0.5914360212837382, 27: 0.5472007608216822, 28: 0.5472007608216822, 29: 0.5256211426551338, 30: 0.5256211426551338, 31: 0.5256211426551338, 32: 0.5256211426551338, 33: 0.5256211426551338, 34: 0.5256211426551338, 35: 0.5256211426551338, 36: 0.5256211426551338, 37: 0.5256211426551338, 38: 0.5256211426551338, 39: 0.5256211426551338, 40: 0.5256211426551338, 41: 0.5256211426551338, 42: 0.5256211426551338, 43: 0.5256211426551338, 44: 0.5256211426551338, 45: 0.5256211426551338, 46: 0.5256211426551338, 47: 0.5256211426551338, 48: 0.5256211426551338, 49: 0.5256211426551338}\n"
     ]
    }
   ],
   "source": [
    "# Optimize our model for encoding 1\n",
    "# > 0.7 for range -18, 15\n",
    "m1e1_scores = optimize_model_1(train_freq)\n",
    "print(m1e1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6387567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-22, 0.7307371503191736)\n",
      "{-50: 0.19592322139785243, -49: 0.19786892021015481, -48: 0.20560262149538758, -47: 0.209440318888223, -46: 0.22272210520423597, -45: 0.23671078279491983, -44: 0.27511989780986246, -43: 0.3691937782364082, -42: 0.38949707572205194, -41: 0.4213513450673115, -40: 0.44000053094245034, -39: 0.4845844003441933, -38: 0.5459330984342273, -37: 0.5720386230511478, -36: 0.6237501450460888, -35: 0.6507780339038156, -34: 0.6669376923329786, -33: 0.6842416694691897, -32: 0.7058165799212046, -31: 0.7087413403996863, -30: 0.7191957757837637, -29: 0.7257433663800557, -28: 0.7228793202342384, -27: 0.7228793202342384, -26: 0.7240759142554543, -25: 0.7229016723120401, -24: 0.728684023973364, -23: 0.728684023973364, -22: 0.7307371503191736, -21: 0.7209688165205819, -20: 0.7207554049707868, -19: 0.7207554049707868, -18: 0.723189332755918, -17: 0.7119239687008364, -16: 0.7119239687008364, -15: 0.7119239687008364, -14: 0.7119239687008364, -13: 0.7119239687008364, -12: 0.7119239687008364, -11: 0.7119239687008364, -10: 0.7119239687008364, -9: 0.7119239687008364, -8: 0.7119239687008364, -7: 0.7119239687008364, -6: 0.710055592680475, -5: 0.710055592680475, -4: 0.710055592680475, -3: 0.710055592680475, -2: 0.710055592680475, -1: 0.710055592680475, 0: 0.710055592680475, 1: 0.710055592680475, 2: 0.710055592680475, 3: 0.710055592680475, 4: 0.710055592680475, 5: 0.710055592680475, 6: 0.710055592680475, 7: 0.710055592680475, 8: 0.710055592680475, 9: 0.710055592680475, 10: 0.6927634527361035, 11: 0.670758097485143, 12: 0.670758097485143, 13: 0.670758097485143, 14: 0.670758097485143, 15: 0.670758097485143, 16: 0.670758097485143, 17: 0.670758097485143, 18: 0.670758097485143, 19: 0.670758097485143, 20: 0.670758097485143, 21: 0.638122770901922, 22: 0.638122770901922, 23: 0.638122770901922, 24: 0.5738440133206277, 25: 0.5738440133206277, 26: 0.5738440133206277, 27: 0.5256211426551338, 28: 0.5256211426551338, 29: 0.5256211426551338, 30: 0.5256211426551338, 31: 0.5256211426551338, 32: 0.5256211426551338, 33: 0.5256211426551338, 34: 0.5256211426551338, 35: 0.5256211426551338, 36: 0.5256211426551338, 37: 0.5256211426551338, 38: 0.5256211426551338, 39: 0.5256211426551338, 40: 0.5256211426551338, 41: 0.5256211426551338, 42: 0.5256211426551338, 43: 0.5256211426551338, 44: 0.5256211426551338, 45: 0.5256211426551338, 46: 0.5256211426551338, 47: 0.5256211426551338, 48: 0.5256211426551338, 49: 0.5256211426551338}\n"
     ]
    }
   ],
   "source": [
    "# Optimize our model for encoding 2\n",
    "# > 0.7 for range -32, 9\n",
    "m1e2_scores = optimize_model_1(train_tf_idf)\n",
    "print(m1e2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee26c4",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "051f5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(sentence : str, midVal, cutoff, bad_words, mid_words):\n",
    "    sexist = 0\n",
    "    for word in sentence.split():\n",
    "        if word in bad_words:\n",
    "            sexist += 10\n",
    "        elif word in mid_words:\n",
    "            sexist += midVal\n",
    "    sexist = 1 if sexist >= cutoff else 0\n",
    "    return sexist\n",
    "\n",
    "def model_2_test_range(bad, mid, f1_scores, clean_X_test, ratio, scp):\n",
    "    bad_words = ratio.loc[ratio >= 1 - scp + bad].index\n",
    "    mid_words = ratio.loc[(1 - scp + bad > ratio) & (ratio > mid)].index\n",
    "    \n",
    "\n",
    "    for midVal in range(0,6):\n",
    "        for cutoff in range(1,8):\n",
    "            predictions = []\n",
    "            for comment in clean_X_test:\n",
    "                predictions.append(model_2(comment, midVal, 5*cutoff, bad_words, mid_words))\n",
    "            f1 = f1_score(Y_test, predictions, average = \"weighted\")\n",
    "            f1_scores[(bad, mid, midVal, cutoff)] = f1\n",
    "            print(str(bad) + \" \" + str(mid) + \" \" + str(midVal) + \" \" + str(cutoff) + \": \" + str(f1), end = \"\\r\")\n",
    "            if(f1 > 0.78):\n",
    "                print(str(bad) + \" \" + str(mid) + \" \" + str(midVal) + \" \" + str(cutoff) + \": \" + str(f1))\n",
    "                \n",
    "\n",
    "def optimize_model_2(word_frequency : pd.DataFrame, badMin, badMax):\n",
    "    freq_with_labels = word_frequency.copy()\n",
    "    freq_with_labels['_label'] = Y_train.tolist()\n",
    "    # sexist_comment_percent = the percentage of comments that are sexist (as a decimal)\n",
    "    sexist_comment_percent = len(Y_train.loc[Y_train== 1]) / len(Y_train)\n",
    "    # common_freq = all words that appear at least 5 times\n",
    "    common_freq = freq_with_labels.sum().loc[freq_with_labels.sum() >= 5]\n",
    "    # sexist = the number of times a word appears in a sexist comment given for all words in common_freq\n",
    "    sexist = word_frequency[freq_with_labels[\"_label\"] == 1].sum().loc[freq_with_labels.sum() >= 5]\n",
    "    # ratio = # of sexist comments / # of comments for all words in common_freq\n",
    "    ratio  = (sexist/common_freq).sort_values()\n",
    "    \n",
    "    f1_scores = {}\n",
    "    clean_X_test = clean_and_lemmatize(X_test)\n",
    "    for bad in range(badMin,badMax + 1):\n",
    "        for mid in range(6, int((1 - sexist_comment_percent + bad/100)*20)):\n",
    "            model_2_test_range(bad/100, 5 * mid/100, f1_scores, clean_X_test, ratio, sexist_comment_percent)\n",
    "    return f1_scores                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4488fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15 0.8 5 7: 0.525621142655133886\r"
     ]
    }
   ],
   "source": [
    "m2e1_scores = optimize_model_2(train_freq, -18, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "523f67ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=2; total time=   7.2s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=2; total time=   4.0s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=5; total time=  17.4s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=5; total time=   7.1s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=2; total time=   3.1s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=5; total time=   3.0s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=5; total time=   7.2s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=2; total time=  10.8s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=2; total time=  18.1s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=5; total time=   7.0s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=2; total time=   3.1s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=5; total time=   3.0s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=5; total time=   7.4s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=2; total time=   7.1s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=2; total time=   3.8s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=2; total time=  18.8s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=5; total time=   6.7s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=5; total time=   4.3s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=2; total time=   2.3s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=2; total time=   5.8s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=2; total time=   3.9s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=2; total time=  10.5s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=5; total time=   3.9s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=2; total time=  12.2s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=5; total time=  10.3s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=5; total time=   4.4s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=5; total time=   2.1s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=2; total time=   6.0s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=2; total time=   3.8s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=5; total time=  10.6s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=5; total time=   3.9s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=2; total time=  12.9s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=2; total time=   7.5s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=5; total time=   4.4s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=5; total time=   2.2s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=2; total time=   6.0s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=2; total time=   3.9s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=5; total time=  10.7s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=5; total time=   4.1s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=5; total time=  12.2s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=5; total time=  10.1s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=2; total time=   4.4s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=5; total time=   2.2s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=5; total time=   6.0s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=5; total time=   3.8s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=2; total time=   7.1s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=5; total time=   6.5s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=5; total time=  18.4s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=2; total time=   3.2s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=2; total time=   2.1s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=2; total time=   7.5s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=5; total time=   3.7s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=2; total time=  10.7s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=2; total time=  17.6s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=2; total time=   7.5s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=5; total time=   3.2s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=2; total time=   2.2s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=5; total time=   7.4s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=5; total time=   3.6s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=5; total time=  10.3s\n",
      "[CV] END bootstrap=True, min_samples_leaf=4, min_samples_split=2; total time=   4.1s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=2; total time=  12.6s\n",
      "[CV] END bootstrap=False, min_samples_leaf=4, min_samples_split=2; total time=   7.6s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=2; total time=   4.4s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=2; total time=   7.4s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=5; total time=   4.9s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=5; total time=   7.0s\n",
      "[CV] END bootstrap=True, min_samples_leaf=2, min_samples_split=5; total time=   6.4s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=5; total time=  17.9s\n",
      "[CV] END bootstrap=True, min_samples_leaf=1, min_samples_split=2; total time=   4.4s\n",
      "[CV] END bootstrap=False, min_samples_leaf=1, min_samples_split=2; total time=   7.4s\n",
      "[CV] END bootstrap=False, min_samples_leaf=2, min_samples_split=5; total time=   5.0s\n",
      "0.09 0.75 5 7: 0.5256211426551338\r"
     ]
    }
   ],
   "source": [
    "m2e2_scores = optimize_model_2(train_tf_idf, -32, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5593f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.09, 0.45, 2, 1), 0.7520494179836824)\n",
      "((-0.22, 0.35, 1, 1), 0.7315814086787903)\n"
     ]
    }
   ],
   "source": [
    "print(max(m2e1_scores.items(), key=lambda x:x[1]))\n",
    "print(max(m2e2_scores.items(), key=lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed8871",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f97c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_forest_classifier(training_features):\n",
    "    # Copy the dataframe, not sure if necessary but wanting avoid state issues\n",
    "    data = training_features.copy()\n",
    "    labels = Y_train\n",
    "    rand_forest = RandomForestClassifier(random_state=0)\n",
    "    rand_forest.fit(data, labels)\n",
    "    return rand_forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b7066",
   "metadata": {},
   "source": [
    "The baseline model performance is pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60292a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline random_forest_models are pretty good\n",
    "word_frequency_rand_forest = create_random_forest_classifier(train_freq)\n",
    "tf_idf_rand_forest = create_random_forest_classifier(train_tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0549e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7254127116092841"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(\n",
    "    Y_test, \n",
    "    word_frequency_rand_forest.predict(word_frequency(X_test)), \n",
    "    average = \"weighted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "293e6a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7236036759267283"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(\n",
    "    Y_test, \n",
    "    tf_idf_rand_forest.predict(TFIDF(X_test)), \n",
    "    average = \"weighted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cb389",
   "metadata": {},
   "source": [
    "But there is some tuning we can still do to improve performance. Note, that the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbe92674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model parameters we want to optimzie.\n",
    "# It's good to optimize on all of them, but it can dramatically increase runtime\n",
    "# Takes about 3 minutes to run with just these\n",
    "# Got this idea from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# Minimum number of samples required to split a node\n",
    "split_samples = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {\n",
    "    'min_samples_split': split_samples,\n",
    "    'min_samples_leaf': min_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "# Number of trees in random forest\n",
    "def find_best_random_forest(x_train):\n",
    "    rf = RandomForestClassifier(random_state=0)\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator = rf, \n",
    "        param_distributions = random_grid, \n",
    "        n_iter = 100, \n",
    "        cv = 3, \n",
    "        verbose=2, \n",
    "        random_state=0, \n",
    "        n_jobs = -1\n",
    "    )# Fit the random search model\n",
    "    rf_random.fit(x_train, Y_train)\n",
    "    return rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a89e8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 12 is smaller than n_iter=100. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "word_frequency_rand_forest_improved = find_best_random_forest(train_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7063093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 12 is smaller than n_iter=100. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    }
   ],
   "source": [
    "tf_idf_rand_forest_improved = find_best_random_forest(train_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7823442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7254127116092841"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance\n",
    "f1_score(\n",
    "    Y_test, \n",
    "    word_frequency_rand_forest_improved.predict(word_frequency(X_test)), \n",
    "    average = \"weighted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54798678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_rand_forest_improved.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1316eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7215546333818647"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_test, tf_idf_rand_forest_improved.predict(TFIDF(X_test)), average = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1828ab3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_rand_forest_improved.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827b394-47a9-4ccc-8ee3-9a9059064cff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. What preprocessing steps do you follow?\n",
    "   \n",
    "   Your answer:\n",
    "   \n",
    "2. How do you select the features from the inputs?\n",
    "   \n",
    "   Your answer:\n",
    "   \n",
    "3. Which model you use and what is the structure of your model?\n",
    "   \n",
    "   Your answer:\n",
    "   \n",
    "4. How do you train your model?\n",
    "   \n",
    "   Your answer:\n",
    "   \n",
    "5. What is the performance of your best model?\n",
    "   \n",
    "   Your answer:\n",
    "   \n",
    "6. What other models or feature engineering methods would you like to implement in the future?\n",
    "   \n",
    "   Your answer:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9a3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
