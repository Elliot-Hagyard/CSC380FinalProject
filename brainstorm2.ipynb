{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/elliothagyard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewire_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sexism2022_english-9609</td>\n",
       "      <td>In Nigeria, if you rape a woman, the men rape ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sexism2022_english-16993</td>\n",
       "      <td>Then, she's a keeper. ðŸ˜‰</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sexism2022_english-13149</td>\n",
       "      <td>This is like the Metallica video where the poo...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sexism2022_english-13021</td>\n",
       "      <td>woman?</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sexism2022_english-966</td>\n",
       "      <td>I bet she wished she had a gun</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  rewire_id  \\\n",
       "0   sexism2022_english-9609   \n",
       "1  sexism2022_english-16993   \n",
       "2  sexism2022_english-13149   \n",
       "3  sexism2022_english-13021   \n",
       "4    sexism2022_english-966   \n",
       "\n",
       "                                                text  label  split  \n",
       "0  In Nigeria, if you rape a woman, the men rape ...      0  train  \n",
       "1                            Then, she's a keeper. ðŸ˜‰      0  train  \n",
       "2  This is like the Metallica video where the poo...      0  train  \n",
       "3                                             woman?      0  train  \n",
       "4                     I bet she wished she had a gun      0  train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "comments_df = pd.read_csv('edos_labelled_data.csv') \n",
    "le = preprocessing.LabelEncoder()\n",
    "comments_df[\"label\"] = le.fit_transform(comments_df[\"label\"])\n",
    "comments_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4193,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_train_df = comments_df[comments_df[\"split\"] == \"train\"]\n",
    "comments_test_df = comments_df[comments_df[\"split\"] == \"test\"]\n",
    "X_train = comments_train_df[\"text\"]\n",
    "Y_train = comments_train_df[\"label\"]\n",
    "X_test = comments_test_df[\"text\"]\n",
    "Y_test = comments_test_df[\"label\"]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comments: list[str]) -> list[str]:\n",
    "    # remove unicode\n",
    "    # remove punct\n",
    "    comments_clean = [comment.encode(\"ascii\", \"ignore\").decode() for comment in comments]\n",
    "    comments_clean = list(map(lambda x : x.lower(), comments_clean))\n",
    "    comments_clean = [re.sub(r'(#\\w+|\\[user\\]|\\[url\\])', '', comment) for comment in comments_clean]\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    comments_clean = [comment.translate(translator) for comment in comments_clean]\n",
    "    #comments_clean = [[wln.lemmatize(word.strip()) for word in comment.split()] for comment in comments_clean]\n",
    "\n",
    "    return comments_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "def toWordFreqDF(x, y):\n",
    "    clean_x = clean(x)\n",
    "    vectorizer = CountVectorizer()\n",
    "    print(type(clean_x))\n",
    "    vec = vectorizer.fit_transform(clean_x)\n",
    "    frequency_df = pd.DataFrame(vec.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    frequency_df['_label'] = y.tolist()\n",
    "    frequency_df['_label'].tail()\n",
    "    return frequency_df\n",
    "\n",
    "train_freq = toWordFreqDF(X_train, Y_train)\n",
    "test_freq = toWordFreqDF(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/elliothagyard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/elliothagyard/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1950s  1950s certain  1950s certain state  510  510 asian  \\\n",
      "0       0.0            0.0                  0.0  0.0        0.0   \n",
      "1       0.0            0.0                  0.0  0.0        0.0   \n",
      "2       0.0            0.0                  0.0  0.0        0.0   \n",
      "3       0.0            0.0                  0.0  0.0        0.0   \n",
      "4       0.0            0.0                  0.0  0.0        0.0   \n",
      "...     ...            ...                  ...  ...        ...   \n",
      "4188    0.0            0.0                  0.0  0.0        0.0   \n",
      "4189    0.0            0.0                  0.0  0.0        0.0   \n",
      "4190    0.0            0.0                  0.0  0.0        0.0   \n",
      "4191    0.0            0.0                  0.0  0.0        0.0   \n",
      "4192    0.0            0.0                  0.0  0.0        0.0   \n",
      "\n",
      "      510 asian lady  abdication  abdication manhood  abhorrent  \\\n",
      "0                0.0         0.0                 0.0        0.0   \n",
      "1                0.0         0.0                 0.0        0.0   \n",
      "2                0.0         0.0                 0.0        0.0   \n",
      "3                0.0         0.0                 0.0        0.0   \n",
      "4                0.0         0.0                 0.0        0.0   \n",
      "...              ...         ...                 ...        ...   \n",
      "4188             0.0         0.0                 0.0        0.0   \n",
      "4189             0.0         0.0                 0.0        0.0   \n",
      "4190             0.0         0.0                 0.0        0.0   \n",
      "4191             0.0         0.0                 0.0        0.0   \n",
      "4192             0.0         0.0                 0.0        0.0   \n",
      "\n",
      "      abhorrent nature  ...  zog  zog support  zog support remain  zombie  \\\n",
      "0                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "1                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "2                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "3                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "...                ...  ...  ...          ...                 ...     ...   \n",
      "4188               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4189               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4190               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4191               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4192               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "\n",
      "      zombie apocalypse  zombie apocalypse todays  zoom  zoom face  \\\n",
      "0                   0.0                       0.0   0.0        0.0   \n",
      "1                   0.0                       0.0   0.0        0.0   \n",
      "2                   0.0                       0.0   0.0        0.0   \n",
      "3                   0.0                       0.0   0.0        0.0   \n",
      "4                   0.0                       0.0   0.0        0.0   \n",
      "...                 ...                       ...   ...        ...   \n",
      "4188                0.0                       0.0   0.0        0.0   \n",
      "4189                0.0                       0.0   0.0        0.0   \n",
      "4190                0.0                       0.0   0.0        0.0   \n",
      "4191                0.0                       0.0   0.0        0.0   \n",
      "4192                0.0                       0.0   0.0        0.0   \n",
      "\n",
      "      zoom face see  _label  \n",
      "0               0.0       0  \n",
      "1               0.0       0  \n",
      "2               0.0       0  \n",
      "3               0.0       0  \n",
      "4               0.0       0  \n",
      "...             ...     ...  \n",
      "4188            0.0       1  \n",
      "4189            0.0       0  \n",
      "4190            0.0       0  \n",
      "4191            0.0       0  \n",
      "4192            0.0       1  \n",
      "\n",
      "[4193 rows x 35033 columns]\n",
      "clinton    0.017090\n",
      "wasnt      0.019992\n",
      "bang       0.025021\n",
      "moron      0.028287\n",
      "jewish     0.028320\n",
      "             ...   \n",
      "cunt       0.798190\n",
      "pussy      0.883289\n",
      "bitch      0.923313\n",
      "whore      0.974127\n",
      "_label     1.000000\n",
      "Length: 471, dtype: float64\n",
      "Index(['woman have', 'men be', 'feminist', 'treat', 'woman be', 'slut', 'cunt',\n",
      "       'pussy', 'bitch', 'whore', '_label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean2EvenCleaner(x):\n",
    "    clean_x = clean(x)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    out = []\n",
    "    for sentence in clean_x:\n",
    "        tokens = pos_tag(word_tokenize(sentence))\n",
    "        tagged = list(map(\n",
    "                lambda x : (x[0], get_wordnet_pos(x[1])),\n",
    "                tokens\n",
    "        ))\n",
    "        \n",
    "        word_and_pos = list(filter(\n",
    "            lambda x : x[1] != '', \n",
    "            tagged\n",
    "        ))\n",
    "\n",
    "        out.append(\" \".join(list(map(lambda x : lemmatizer.lemmatize(x[0], x[1]), word_and_pos))))\n",
    "    return out\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDF(x, y):\n",
    "    cleaned = clean2EvenCleaner(x)\n",
    "    cleaned_sexist = clean2EvenCleaner(x[y==1])\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1,3)\n",
    "    )\n",
    "\n",
    "    vectorizer.fit(cleaned_sexist)\n",
    "    out = vectorizer.transform(cleaned)\n",
    "    df = pd.DataFrame(out.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    df['_label'] = y.tolist()\n",
    "    df['_label'].tail()\n",
    "    return df\n",
    "\n",
    "\n",
    "def secondParsing(x, y):\n",
    "    vectorizer = CountVectorizer()\n",
    "    out = clean2EvenCleaner(x)\n",
    "    vec = vectorizer.fit_transform(out)\n",
    "    df = pd.DataFrame(vec.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    df['_label'] = y.tolist()\n",
    "    df['_label'].tail()\n",
    "    return df\n",
    "train_freq2 = secondParsing(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7862923389870593\n",
      "0.8038727340894786\n",
      "      1950s  1950s certain  1950s certain state  510  510 asian  \\\n",
      "0       0.0            0.0                  0.0  0.0        0.0   \n",
      "1       0.0            0.0                  0.0  0.0        0.0   \n",
      "2       0.0            0.0                  0.0  0.0        0.0   \n",
      "3       0.0            0.0                  0.0  0.0        0.0   \n",
      "4       0.0            0.0                  0.0  0.0        0.0   \n",
      "...     ...            ...                  ...  ...        ...   \n",
      "4188    0.0            0.0                  0.0  0.0        0.0   \n",
      "4189    0.0            0.0                  0.0  0.0        0.0   \n",
      "4190    0.0            0.0                  0.0  0.0        0.0   \n",
      "4191    0.0            0.0                  0.0  0.0        0.0   \n",
      "4192    0.0            0.0                  0.0  0.0        0.0   \n",
      "\n",
      "      510 asian lady  abdication  abdication manhood  abhorrent  \\\n",
      "0                0.0         0.0                 0.0        0.0   \n",
      "1                0.0         0.0                 0.0        0.0   \n",
      "2                0.0         0.0                 0.0        0.0   \n",
      "3                0.0         0.0                 0.0        0.0   \n",
      "4                0.0         0.0                 0.0        0.0   \n",
      "...              ...         ...                 ...        ...   \n",
      "4188             0.0         0.0                 0.0        0.0   \n",
      "4189             0.0         0.0                 0.0        0.0   \n",
      "4190             0.0         0.0                 0.0        0.0   \n",
      "4191             0.0         0.0                 0.0        0.0   \n",
      "4192             0.0         0.0                 0.0        0.0   \n",
      "\n",
      "      abhorrent nature  ...  zog  zog support  zog support remain  zombie  \\\n",
      "0                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "1                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "2                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "3                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4                  0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "...                ...  ...  ...          ...                 ...     ...   \n",
      "4188               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4189               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4190               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4191               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "4192               0.0  ...  0.0          0.0                 0.0     0.0   \n",
      "\n",
      "      zombie apocalypse  zombie apocalypse todays  zoom  zoom face  \\\n",
      "0                   0.0                       0.0   0.0        0.0   \n",
      "1                   0.0                       0.0   0.0        0.0   \n",
      "2                   0.0                       0.0   0.0        0.0   \n",
      "3                   0.0                       0.0   0.0        0.0   \n",
      "4                   0.0                       0.0   0.0        0.0   \n",
      "...                 ...                       ...   ...        ...   \n",
      "4188                0.0                       0.0   0.0        0.0   \n",
      "4189                0.0                       0.0   0.0        0.0   \n",
      "4190                0.0                       0.0   0.0        0.0   \n",
      "4191                0.0                       0.0   0.0        0.0   \n",
      "4192                0.0                       0.0   0.0        0.0   \n",
      "\n",
      "      zoom face see  _label  \n",
      "0               0.0       0  \n",
      "1               0.0       0  \n",
      "2               0.0       0  \n",
      "3               0.0       0  \n",
      "4               0.0       0  \n",
      "...             ...     ...  \n",
      "4188            0.0       1  \n",
      "4189            0.0       0  \n",
      "4190            0.0       0  \n",
      "4191            0.0       0  \n",
      "4192            0.0       1  \n",
      "\n",
      "[4193 rows x 35033 columns]\n",
      "clinton    0.017090\n",
      "wasnt      0.019992\n",
      "bang       0.025021\n",
      "moron      0.028287\n",
      "jewish     0.028320\n",
      "             ...   \n",
      "cunt       0.798190\n",
      "pussy      0.883289\n",
      "bitch      0.923313\n",
      "whore      0.974127\n",
      "_label     1.000000\n",
      "Length: 471, dtype: float64\n",
      "Index(['hot', 'imagine', 'arent', 'woman', 'easy', 'almost', 'wish', 'ugly',\n",
      "       'fat', 'buy', 'cause', 'men', 'chad', 'be woman', 'be more',\n",
      "       'woman have', 'men be', 'feminist', 'treat', 'woman be', 'slut', 'cunt',\n",
      "       'pussy', 'bitch', 'whore', '_label'],\n",
      "      dtype='object')\n",
      "0.7311916677697217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score \n",
    "sexist_comment_percent = len(Y_train.loc[Y_train== 1]) / len(Y_train) \n",
    "common_freq2 = train_freq2.sum().loc[train_freq2.sum() >= 5]\n",
    "sexist = train_freq2[train_freq2[\"_label\"] == 1].sum().loc[train_freq2.sum() >= 5]\n",
    "ratio  = sexist/common_freq2.sort_values()\n",
    "bad_words = ratio.loc[ratio > 1 - sexist_comment_percent + .02].index\n",
    "\n",
    "mid_words = ratio.loc[\n",
    "    (1 - sexist_comment_percent + .02 > ratio) \n",
    "    & (ratio > 0.424)].index\n",
    "\n",
    "def test_word(sentence : str):\n",
    "    sexist = 0\n",
    "    for word in sentence.split():\n",
    "        if word in bad_words:\n",
    "            sexist = 1\n",
    "    return sexist\n",
    "\n",
    "def test_word2(sentence : str, midVal, cutoff):\n",
    "    sexist = 0\n",
    "    for word in sentence.split():\n",
    "        if word in bad_words:\n",
    "            sexist += 10\n",
    "        elif word in mid_words:\n",
    "            sexist += midVal\n",
    "    sexist = 1 if sexist >= cutoff else 0\n",
    "    return sexist\n",
    "\n",
    "predict3 = []\n",
    "print()\n",
    "for comment in clean2EvenCleaner(X_test):\n",
    "    predict3.append(test_word(comment))\n",
    "\n",
    "print(f1_score(Y_test, predict3, average = \"weighted\"))\n",
    "predictions = []\n",
    "for comment in clean2EvenCleaner(X_test):\n",
    "    predictions.append(test_word2(comment, 3, 10))\n",
    "print(f1_score(Y_test, predictions, average = \"weighted\"))\n",
    "pred_new = []\n",
    "a = TFIDF(X_train, Y_train)\n",
    "print(a)\n",
    "sexist_comment_percent = len(Y_train.loc[Y_train== 1]) / len(Y_train) \n",
    "common_freq2 = a.sum().loc[a.sum() >= 3]\n",
    "sexist = a[a[\"_label\"] == 1].sum().loc[a.sum() >= 3]\n",
    "ratio  = (sexist/common_freq2).sort_values()\n",
    "print(ratio)\n",
    "bad_words = ratio.loc[ratio > 1 - sexist_comment_percent- .3].index\n",
    "print(bad_words)\n",
    "for comment in clean2EvenCleaner(X_test):\n",
    "    pred_new.append(test_word2(comment, 3, 10))\n",
    "print(f1_score(Y_test, pred_new, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_label', 'behaviour', 'bitch', 'cock', 'cunt', 'earn', 'exploit',\n",
       "       'feminazi', 'hag', 'happiness', 'hoe', 'hole', 'honest', 'hypergamy',\n",
       "       'logic', 'loyal', 'misogyny', 'oppress', 'pussy', 'resign', 'roast',\n",
       "       'roastie', 'robot', 'romance', 'screw', 'shower', 'skank', 'slut',\n",
       "       'slutty', 'status', 'strength', 'string', 'thots', 'tit', 'tranny',\n",
       "       'virginity', 'west', 'whore', 'yo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "189\n",
      "0.7754395239894935\n"
     ]
    }
   ],
   "source": [
    "sexist_comment_percent = len(Y_train.loc[Y_train== 1]) / len(Y_train) \n",
    "common_freq = train_freq.sum().loc[train_freq.sum() >= 3]\n",
    "sexist = train_freq[train_freq[\"_label\"] == 1].sum().loc[train_freq.sum() >= 3]\n",
    "ratio  = sexist/common_freq.sort_values()\n",
    "bad_words = ratio.loc[ratio > 1 - sexist_comment_percent + .05].index\n",
    "\n",
    "def test_word(sentence : str):\n",
    "    sexist = 0\n",
    "    for word in sentence.split():\n",
    "        if word in bad_words:\n",
    "            sexist = 1\n",
    "            \n",
    "    return sexist\n",
    "\n",
    "predict = []\n",
    "print()\n",
    "for comment in clean(X_test):\n",
    "    predict.append(test_word(comment))\n",
    "print(sum(predict))\n",
    "print(bad_words)\n",
    "print(bad_words)\n",
    "print(f1_score(Y_test, predict, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['17', '40', 'above', 'alpha', 'attention', 'british', 'bs', 'chad',\n",
      "       'charge', 'cheating', 'club', 'completely', 'confident', 'disgusting',\n",
      "       'equal', 'exist', 'exposed', 'failure', 'fantasy', 'feminism',\n",
      "       'finally', 'greatest', 'gross', 'hahaha', 'handle', 'higher',\n",
      "       'immediately', 'learn', 'lifetime', 'loved', 'low', 'lying', 'market',\n",
      "       'marriage', 'mode', 'modern', 'motherhood', 'movies', 'normies',\n",
      "       'obese', 'opposite', 'option', 'patriarchy', 'physically', 'rich',\n",
      "       'screw', 'simp', 'sluts', 'species', 'stick', 'successful', 'sucked',\n",
      "       'tits', 'tons', 'tranny', 'treat', 'turns', 'typical', 'unfortunately',\n",
      "       'validation', 'victims', 'west', 'whenever', 'wing'],\n",
      "      dtype='object')\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score \n",
    "\n",
    "mid_words = ratio.loc[\n",
    "    (1 - sexist_comment_percent + .05 > ratio) \n",
    "    & (ratio > 0.6)\n",
    "].index\n",
    "print(mid_words)\n",
    "good_words = ratio.loc[sexist_comment_percent - .05 > ratio].index\n",
    "\n",
    "def test_word2(sentence : str, badVal, midVal, cutoff):\n",
    "    sexist = 0\n",
    "    for word in sentence.split():\n",
    "        if word in bad_words:\n",
    "            sexist += badVal\n",
    "        elif word in mid_words:\n",
    "            sexist += midVal\n",
    "    sexist = 1 if sexist >= cutoff else 0\n",
    "    return sexist\n",
    "\n",
    "predict2 = []\n",
    "best_F1 = [0]\n",
    "def find_best_variables():\n",
    "    global best_F1\n",
    "    for bad in range(0, 0): # 40, 5 3 5 F1 = 0.7904021016988467\n",
    "        for mid in range(0, bad):\n",
    "            for cut in range(5, 100):\n",
    "                predict2 = []\n",
    "                for comment in clean(X_test):\n",
    "                    predict2.append(test_word2(comment, (bad/10), (mid/10), (cut/10)))\n",
    "                f1 = f1_score(Y_test, predict2, average = \"weighted\")\n",
    "                print(str(bad) + \" \" + str(mid) + \" \" + str(cut), end = \"\\r\")\n",
    "                if(f1 > best_F1[0]):\n",
    "                    best_F1 = []\n",
    "                    best_F1.append(f1)\n",
    "                    best_F1.append([bad, mid, cut])\n",
    "                    print(str(bad) + \" \" + str(mid) + \" \" + str(cut) + \" F1 = \" + str(f1))\n",
    "find_best_variables()\n",
    "print(best_F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#Our_X_Train, Our_X_Test = train_test_split(X_train, test_size=0.2, shuffle=False)\n",
    "#Our_Y_Train, Our_Y_Test = train_test_split(Y_train, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7789432354510586 0.0 0.30000000000000004 0 5\n",
      "0.7829160467761055 0.0 0.35000000000000003 1 9\n",
      "0.792841730559442 0.0 0.35000000000000003 1 10\n",
      "0.7975281156078042 0.0 0.35000000000000003 1 11\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/x4/xf2q_cf56pg5kt_qpw239_800000gn/T/ipykernel_52057/1302212820.py\", line 39, in <module>\n",
      "    print(find_best_ranges())\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/x4/xf2q_cf56pg5kt_qpw239_800000gn/T/ipykernel_52057/1302212820.py\", line 6, in find_best_ranges\n",
      "    test_range(bad/100, 0.05 * mid, f1_scores, best_f1)\n",
      "  File \"/var/folders/x4/xf2q_cf56pg5kt_qpw239_800000gn/T/ipykernel_52057/1302212820.py\", line 21, in test_range\n",
      "    predictions.append(test_word3(comment, midVal, cutoff, bad_words, mid_words))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/x4/xf2q_cf56pg5kt_qpw239_800000gn/T/ipykernel_52057/1302212820.py\", line -1, in test_word3\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "def find_best_ranges():\n",
    "    f1_scores = []\n",
    "    best_f1 = [0]\n",
    "    for bad in range(0,15):\n",
    "        for mid in range(6, int((1 - sexist_comment_percent + bad/100)*20)):\n",
    "            test_range(bad/100, 0.05 * mid, f1_scores, best_f1)\n",
    "    return f1_scores\n",
    "\n",
    "def test_range(bad, mid, f1_scores, best_f1):\n",
    "    bad_words = ratio.loc[ratio >= 1 - sexist_comment_percent + bad].index\n",
    "    mid_words = ratio.loc[\n",
    "    (1 - sexist_comment_percent + bad > ratio) \n",
    "    & (ratio > mid)].index\n",
    "    \n",
    "\n",
    "    for midVal in range(0,6):\n",
    "        for cutoff in range(5,31):\n",
    "            print(str(bad) + \" \" + str(mid) + \" \" + str(midVal) + \" \" + str(cutoff) + \" \", end = \"\\r\")\n",
    "            predictions = []\n",
    "            for comment in clean(X_train):\n",
    "                predictions.append(test_word3(comment, midVal, cutoff, bad_words, mid_words))\n",
    "            f1 = f1_score(Y_train, predictions, average = \"weighted\")\n",
    "            f1_scores.append([f1, bad, mid, midVal, cutoff])\n",
    "            if(f1 > best_f1[0]):\n",
    "                best_f1[0] = f1\n",
    "                print(str(f1) + \" \" + str(bad) + \" \" + str(mid) + \" \" + str(midVal) + \" \" + str(cutoff))\n",
    "                \n",
    "    \n",
    "def test_word3(sentence : str, midVal, cutoff, bad_words, mid_words):\n",
    "    sexist = 0\n",
    "    for word in sentence.split():\n",
    "        if word in bad_words:\n",
    "            sexist += 10\n",
    "        elif word in mid_words:\n",
    "            sexist += midVal\n",
    "    sexist = 1 if sexist >= cutoff else 0\n",
    "    return sexist\n",
    "\n",
    "print(find_best_ranges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7789432354510586 -0.01 0.30000000000000004 0 5\n",
      "0.7829160467761055 -0.01 0.35000000000000003 1 9\n",
      "0.792841730559442 -0.01 0.35000000000000003 1 10\n",
      "0.7975281156078042 -0.01 0.35000000000000003 1 11\n",
      "0.8118695898697101 -0.01 0.4 1 5\n",
      "0.8161997121844654 -0.01 0.45 2 5\n",
      "-0.03 0.45 3 15 000000003 5 29 \r"
     ]
    }
   ],
   "source": [
    "def find_best_ranges_neg():\n",
    "    f1_scores = []\n",
    "    best_f1 = [0]\n",
    "    for bad in range(1,15):\n",
    "        for mid in range(6, int((1 - sexist_comment_percent + bad/100)*20)):\n",
    "            test_range(bad/-100, 0.05 * mid, f1_scores, best_f1)\n",
    "    return f1_scores\n",
    "\n",
    "neg_f1_scores = find_best_ranges_neg()\n",
    "print(neg_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7880027200175773\n"
     ]
    }
   ],
   "source": [
    "# 0, 0.45, 2, 5\n",
    "\n",
    "bad_words = ratio.loc[ratio >= 1 - sexist_comment_percent - 0.01].index\n",
    "mid_words = ratio.loc[(1 - sexist_comment_percent - 0.01 > ratio) \n",
    "    & (ratio > 0.45)].index\n",
    "\n",
    "predictions = []\n",
    "for comment in clean(X_test):\n",
    "    predictions.append(test_word3(comment, 2, 5, bad_words, mid_words))\n",
    "    \n",
    "print(f1_score(Y_test, predictions, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84       789\n",
      "           1       0.00      0.00      0.00       297\n",
      "\n",
      "    accuracy                           0.73      1086\n",
      "   macro avg       0.36      0.50      0.42      1086\n",
      "weighted avg       0.53      0.73      0.61      1086\n",
      "\n",
      "RAND\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       789\n",
      "           1       0.69      0.44      0.53       297\n",
      "\n",
      "    accuracy                           0.79      1086\n",
      "   macro avg       0.75      0.68      0.70      1086\n",
      "weighted avg       0.78      0.79      0.78      1086\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88       789\n",
      "           1       0.79      0.40      0.53       297\n",
      "\n",
      "    accuracy                           0.81      1086\n",
      "   macro avg       0.80      0.68      0.71      1086\n",
      "weighted avg       0.81      0.81      0.78      1086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/elliothagyard/opt/anaconda3/envs/CSC_380/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def eval_predictions(pred):\n",
    "    print(classification_report(Y_test, pred))\n",
    "\n",
    "import random\n",
    "baseline = [0] * len(Y_test)\n",
    "baseline2 = [int(random.uniform(0, 1) > .8) for i in predict]\n",
    "\n",
    "print(\"BASE\")\n",
    "eval_predictions(baseline)\n",
    "print(\"RAND\")\n",
    "# eval_predictions(baseline2)\n",
    "# print(\"BAD\")\n",
    "eval_predictions(predict)\n",
    "# print(\"MID\")\n",
    "# eval_predictions(predictions)\n",
    "eval_predictions(predict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
